# Google Colab Guide

## ðŸš€ Quick Start

### Cell 1: Setup
```python
# Clone repository
!git clone https://github.com/your-username/Finetune-PhoBERT.git

# QUAN TRá»ŒNG: CD vÃ o project (dÃ¹ng % khÃ´ng pháº£i !)
%cd /content/Finetune-PhoBERT

# Install dependencies
!pip install -q -r requirements.txt
```

### Cell 1.5 (OPTIONAL): Prepare VOZ-HSD Dataset
```python
# Skip this if you already have data in data/ folder
# This downloads and prepares balanced VOZ-HSD dataset

%cd /content/Finetune-PhoBERT/data

# Option A: Default (70-30, probsâ‰¥0.95) - Recommended
!python prepare_voz_balanced.py

# Option B: More conservative (80-20, better UX)
# !python prepare_voz_balanced.py --confidence 0.95 --ratio 80

# Option C: Higher recall (60-40, catch more hate)
# !python prepare_voz_balanced.py --confidence 0.90 --ratio 60

# Check summary
!cat dataset_summary.txt

%cd /content/Finetune-PhoBERT
```

**Expected time:** 5-10 minutes for download + processing  
**Output:** train.csv, valid.csv, test.csv (~547K total samples)

### Cell 2: Check Data
```python
import pandas as pd

train_df = pd.read_csv('data/train.csv')
valid_df = pd.read_csv('data/valid.csv')
test_df = pd.read_csv('data/test.csv')

print(f"Train: {len(train_df)} samples")
print(f"Valid: {len(valid_df)} samples")
print(f"Test: {len(test_df)} samples")
print(f"\nLabel distribution:\n{train_df['label'].value_counts()}")
```

### Cell 3: Train Model
```python
# Train (takes ~15-30 mins with GPU)
!python src/train.py
```

### Cell 4: Visualize Training
```python
# Generate training curves
!python src/plot_training.py

# Display
from IPython.display import Image, display
display(Image('out/metrics/training_curves.png'))
```

### Cell 5: Evaluate
```python
# Run evaluation (generates all plots)
!python src/evaluation.py
```

### Cell 6: View Results
```python
from IPython.display import Image, display

# Confusion Matrix
print("Confusion Matrix (Normalized):")
display(Image('out/metrics/confusion_matrix_norm.png'))

# ROC Curve
print("\nROC Curve:")
display(Image('out/metrics/roc_curve.png'))

# PR Curve (important for imbalanced data!)
print("\nPrecision-Recall Curve:")
display(Image('out/metrics/pr_curve.png'))

# Threshold Analysis
print("\nThreshold Analysis:")
display(Image('out/metrics/threshold_analysis.png'))

# Prediction Distribution
print("\nPrediction Distribution:")
display(Image('out/metrics/prediction_dist.png'))
```

### Cell 7: View Report
```python
# Training report
!cat out/metrics/training_report.md

# Metrics JSON
import json
with open('out/metrics/report.json', 'r') as f:
    print(json.dumps(json.load(f), indent=2))
```

### Cell 8: Download Results
```python
# Zip all results
!zip -r results.zip out/metrics/ models/phobert-moderation/

# Download
from google.colab import files
files.download('results.zip')
```

---

## âš ï¸ Common Issues & Warnings

### Normal Warnings (OK to ignore)
```
E0000 00:00:xxx ... Unable to register cuFFT factory...
E0000 00:00:xxx ... Unable to register cuDNN factory...
FutureWarning: `tokenizer` is deprecated...
```
**These are normal** - TensorFlow backend warnings vÃ  deprecation warnings. Code váº«n cháº¡y bÃ¬nh thÆ°á»ng!

### Issue 1: `TypeError: evaluation_strategy` 
**Fixed!** Code Ä‘Ã£ update cho transformers má»›i.

### Issue 2: `TypeError: compute_loss() got unexpected keyword` 
**Fixed!** Code Ä‘Ã£ tÆ°Æ¡ng thÃ­ch vá»›i transformers>=4.35.0.

### Issue 3: `FileNotFoundError: data/`
**Solution**: Cháº¯c cháº¯n Ä‘Ã£ cháº¡y `%cd /content/Finetune-PhoBERT` trÆ°á»›c!

### Issue 4: CUDA out of memory
**Solutions**:
- Giáº£m `BATCH_TRAIN` trong `src/train.py` (line 27)
- Giáº£m `MAX_LENGTH` (line 26)
- DÃ¹ng Runtime > Change runtime type > T4 GPU

### Issue 5: Session timeout
**Solution**: 
- Download model thÆ°á»ng xuyÃªn
- Hoáº·c mount Google Drive:
```python
from google.colab import drive
drive.mount('/content/drive')
# Sau Ä‘Ã³ copy káº¿t quáº£ vÃ o Drive
!cp -r out/ /content/drive/MyDrive/phobert-results/
```

---

## ðŸŽ¯ Tips

1. **Enable GPU**: Runtime > Change runtime type > GPU (T4)
2. **Monitor training**: Training logs hiá»ƒn thá»‹ realtime
3. **Save checkpoints**: Model tá»± Ä‘á»™ng lÆ°u best checkpoint
4. **Download early**: Download results ngay sau train Ä‘á»ƒ trÃ¡nh máº¥t data

## âš¡ GPU Optimization (T4) - AGGRESSIVE! ðŸ”¥

**Code Ä‘Ã£ Ä‘Æ°á»£c optimize Máº NH Ä‘á»ƒ Äƒn Ä‘áº§y VRAM!**
- âœ… Batch size MAX (32/64) - no more 2.7GB waste!
- âœ… FP16 mixed precision
- âœ… Gradient checkpointing OFF (for speed)
- âœ… Fused optimizer
- âœ… Parallel data loading

**Expected:**
- VRAM usage: **~12-13GB / 16GB** (80%+) âœ…
- Training speed: **~8-10 phÃºt** cho 24K samples (3 epochs) ðŸš€
- GPU utilization: **85-95%**

### Monitor GPU (IMPORTANT):
```python
!watch -n 1 nvidia-smi  # Live monitoring (Ctrl+C to stop)
```

**Should see:**
- Memory: **12-13GB / 16GB** (not 2.7GB!)
- GPU Util: **85-95%**

### If OOM (out of memory):
Edit `src/train.py` lines 28-29:
```python
BATCH_TRAIN = 24  # Giáº£m tá»« 32
BATCH_EVAL = 48   # Giáº£m tá»« 64
```

### If VRAM < 10GB (still wasting):
```python
BATCH_TRAIN = 40  # TÄƒng lÃªn!
BATCH_EVAL = 80
```

Chi tiáº¿t: Xem [GPU_OPTIMIZATION.md](GPU_OPTIMIZATION.md)

---

## ðŸ“Š Expected Output Structure

```
/content/Finetune-PhoBERT/
â”œâ”€â”€ out/metrics/
â”‚   â”œâ”€â”€ training_report.json
â”‚   â”œâ”€â”€ training_report.md
â”‚   â”œâ”€â”€ train_log.ndjson
â”‚   â”œâ”€â”€ training_curves.png
â”‚   â”œâ”€â”€ confusion_matrix.png
â”‚   â”œâ”€â”€ confusion_matrix_norm.png
â”‚   â”œâ”€â”€ roc_curve.png
â”‚   â”œâ”€â”€ pr_curve.png
â”‚   â”œâ”€â”€ threshold_analysis.png
â”‚   â”œâ”€â”€ prediction_dist.png
â”‚   â””â”€â”€ report.json
â””â”€â”€ models/phobert-moderation/
    â”œâ”€â”€ config.json
    â”œâ”€â”€ pytorch_model.bin
    â”œâ”€â”€ tokenizer_config.json
    â””â”€â”€ ...
```

All files ready for your research report! ðŸŽ‰

