# HÆ°á»›ng dáº«n sá»­ dá»¥ng - Training Report System

## ğŸ“‹ Tá»•ng quan

Há»‡ thá»‘ng Ä‘Ã£ Ä‘Æ°á»£c nÃ¢ng cáº¥p vá»›i cÃ¡c cÃ´ng cá»¥ visualization vÃ  reporting toÃ n diá»‡n cho bÃ¡o cÃ¡o nghiÃªn cá»©u.

## ğŸš€ CÃ i Ä‘áº·t

```bash
# CÃ i Ä‘áº·t dependencies (Ä‘Ã£ thÃªm seaborn)
pip install -r requirements.txt
```

## ğŸ“Š Workflow

### 1. Training Model

```bash
cd src
python train.py
```

**Output sau khi train:**
- Model Ä‘Æ°á»£c lÆ°u táº¡i: `models/phobert-moderation/`
- Training logs: `out/metrics/train_log.ndjson`
- **Training report**: `out/metrics/training_report.json` vÃ  `training_report.md`

### 2. Visualize Training Curves

```bash
cd src
python plot_training.py
```

**Output:**
- `out/metrics/training_curves.png` - Loss curves & metrics evolution

### 3. Evaluate trÃªn Test Set

```bash
cd src
python evaluation.py
```

**Output:**
- `out/metrics/confusion_matrix.png` - Confusion matrix (counts)
- `out/metrics/confusion_matrix_norm.png` - Confusion matrix (normalized %)
- `out/metrics/roc_curve.png` - ROC curve vá»›i AUC
- `out/metrics/pr_curve.png` - **Precision-Recall curve** (quan trá»ng!)
- `out/metrics/threshold_analysis.png` - Metrics @ different thresholds
- `out/metrics/prediction_dist.png` - Prediction confidence distribution
- `out/metrics/report.json` - Test metrics JSON

## ğŸ“ Cáº¥u trÃºc Output

```
out/metrics/
â”œâ”€â”€ training_report.json       # Training config & metrics (JSON)
â”œâ”€â”€ training_report.md          # Human-readable report (Markdown)
â”œâ”€â”€ train_log.ndjson           # Raw training logs
â”œâ”€â”€ training_curves.png         # Training visualization
â”œâ”€â”€ confusion_matrix.png        # CM counts
â”œâ”€â”€ confusion_matrix_norm.png   # CM percentages
â”œâ”€â”€ roc_curve.png              # ROC with threshold marker
â”œâ”€â”€ pr_curve.png               # PR curve (for imbalanced data)
â”œâ”€â”€ threshold_analysis.png      # Find optimal threshold
â”œâ”€â”€ prediction_dist.png         # Confidence distribution
â””â”€â”€ report.json                # Test set metrics
```

## ğŸ¯ Key Features

### 1. **Training Report** (`training_report.md`)

Bao gá»“m:
- âœ… Hyperparameters (learning rate, batch size, epochs, etc.)
- âœ… Data statistics (class distribution cho train/val/test)
- âœ… Class weights (náº¿u cÃ³)
- âœ… Training summary (loss curves)
- âœ… Best validation metrics
- âœ… Test set results

### 2. **Enhanced Visualizations**

#### Confusion Matrix
- 2 versions: counts + normalized percentages
- Color-coded vá»›i seaborn palette
- Annotations rÃµ rÃ ng

#### ROC Curve
- Grid Ä‘á»ƒ dá»… Ä‘á»c
- Mark threshold hiá»‡n táº¡i
- AUC score

#### **PR Curve (Má»šI)**
- **Critical cho imbalanced data!**
- Shows precision-recall tradeoff
- Baseline comparison

#### **Threshold Analysis (Má»šI)**
- Plot metrics @ 101 thresholds (0.0 to 1.0)
- Tá»± Ä‘á»™ng tÃ¬m best F1 threshold
- So sÃ¡nh vá»›i default 0.5

#### **Prediction Distribution (Má»šI)**
- Histogram of prediction scores
- Separated by true class
- Helps understand model confidence

### 3. **Training Curves**

- Loss evolution (train + validation)
- Metrics evolution (accuracy, precision, recall, F1)
- Mark best checkpoint
- Annotations rÃµ rÃ ng

## ğŸ”§ Customization

### Thay Ä‘á»•i threshold (evaluation)

```bash
THRESHOLD=0.6 python evaluation.py
```

### Modify configs (training)

Edit `src/train.py`:
```python
MODEL_NAME = "vinai/phobert-base-v2"
MAX_LENGTH = 192
BATCH_TRAIN = 8
EPOCHS = 3
LR = 2e-5
```

## ğŸ“ Cho bÃ¡o cÃ¡o nghiÃªn cá»©u

### Files cáº§n thiáº¿t:

1. **Training Report**: `out/metrics/training_report.md` 
   - Copy trá»±c tiáº¿p vÃ o bÃ¡o cÃ¡o hoáº·c paper appendix

2. **Visualizations**:
   - `training_curves.png` - QuÃ¡ trÃ¬nh training
   - `confusion_matrix_norm.png` - Confusion matrix %
   - `pr_curve.png` - Performance cho imbalanced data
   - `threshold_analysis.png` - Threshold tuning

3. **Metrics JSON**: `training_report.json` + `report.json`
   - Parse Ä‘á»ƒ táº¡o LaTeX tables

### LaTeX Table Example:

```latex
\begin{table}[h]
\centering
\caption{Test Set Performance}
\begin{tabular}{lc}
\hline
Metric & Score \\
\hline
Accuracy & 0.XXX \\
Precision (invalid) & 0.XXX \\
Recall (invalid) & 0.XXX \\
F1 Score (invalid) & 0.XXX \\
ROC AUC & 0.XXX \\
PR AUC & 0.XXX \\
\hline
\end{tabular}
\end{table}
```

## âš ï¸ Notes

- Seaborn warning khi chÆ°a install: bÃ¬nh thÆ°á»ng, cháº¡y `pip install seaborn`
- Training curves cáº§n cÃ³ `train_log.ndjson` tá»« quÃ¡ trÃ¬nh training
- Evaluation cáº§n cÃ³ trained model táº¡i `models/phobert-moderation/`

## ğŸ‰ Done!

BÃ¢y giá» báº¡n cÃ³ Ä‘áº§y Ä‘á»§ metrics vÃ  visualizations cho bÃ¡o cÃ¡o!

