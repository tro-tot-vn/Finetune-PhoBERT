# Finetune-PhoBERT

Fine-tuning PhoBERT cho Vietnamese text moderation (binary classification: valid/invalid).

## ğŸ“¦ Features

- âœ… Fine-tune PhoBERT-base-v2 cho binary classification
- âœ… Weighted loss cho imbalanced data
- âœ… Early stopping vá»›i best checkpoint selection
- âœ… Comprehensive training reports & visualizations
- âœ… Multiple evaluation metrics (ROC, PR curve, threshold analysis)
- âš¡ GPU optimized (2x faster on T4: ~12-15 min for 24K samples)

## ğŸš€ Quick Start

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Train model
cd src && python train.py

# 3. Visualize training
python plot_training.py

# 4. Evaluate on test set
python evaluation.py
```

## ğŸ“Š Outputs

Training sáº½ táº¡o:
- **Model**: `models/phobert-moderation/`
- **Training report**: `out/metrics/training_report.md`
- **Visualizations**: `out/metrics/*.png` (8 plots)
- **Metrics**: `out/metrics/*.json`

Chi tiáº¿t xem [USAGE.md](USAGE.md)

## ğŸ“ Structure

```
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.csv
â”‚   â”œâ”€â”€ valid.csv
â”‚   â”œâ”€â”€ test.csv
â”‚   â””â”€â”€ normalize_labels_flex.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ train.py              # Training script
â”‚   â”œâ”€â”€ evaluation.py         # Evaluation + visualization
â”‚   â”œâ”€â”€ callbacks.py          # Custom callbacks
â”‚   â”œâ”€â”€ report_generator.py   # Report generation
â”‚   â””â”€â”€ plot_training.py      # Training curves
â””â”€â”€ requirements.txt
```

## ğŸ”§ Configuration

Edit hyperparameters in `src/train.py`:
- Model: `vinai/phobert-base-v2`
- Max length: 192 tokens
- Batch size: 8 (train) / 16 (eval)
- Epochs: 3
- Learning rate: 2e-5

## ğŸ“ For Research Papers

See [USAGE.md](USAGE.md) for detailed instructions on:
- Extracting metrics for LaTeX tables
- Selecting visualizations for papers
- Understanding output files

## License

MIT
